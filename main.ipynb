{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sihao\\Documents\\projects\\babel\\venv_babel\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import datetime\n",
    "from time import sleep\n",
    "import openai\n",
    "import tiktoken\n",
    "from blingfire import text_to_sentences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATABASE = os.getcwd() + '/documents'\n",
    "CHUNK_SIZE = 5 # number of sentences per paragraph\n",
    "CHUNK_OVERLAP = 1 # number of sentences to overlap between paragraphs \n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "EMBEDDING_TOKEN_LIMIT = 8191-2 # account for the 2 tokens added by the model\n",
    "EMBEDDING_LENGTH = 1536 # length of the embedding vector\n",
    "EMBEDDING_BATCH_SIZE = 25 # number of paras to embed at once\n",
    "\n",
    "ACTIVE_MODEL = \"gpt-3.5-turbo\"\n",
    "ACTIVE_TOKEN_LIMIT = 4096-2\n",
    "\n",
    "# initialize a logger\n",
    "logging.basicConfig(filename=f'{os.getcwd()}/logs/babel.log', \n",
    "                                format='%(asctime)s-%(levelname)s-%(message)s', \n",
    "                                level=logging.INFO)\n",
    "\n",
    "# read API key from secrets directory - bring your own key\n",
    "with open(os.getcwd() + '/secrets/keys.json') as f:\n",
    "    keys = json.load(f)\n",
    "openai.api_key = keys['OPENAI_API_KEY']\n",
    "PINECONE_API_KEY = keys['PINECONE_API_KEY']\n",
    "\n",
    "# initialize a tiktoken encoder for token estimation\n",
    "enc = tiktoken.encoding_for_model(EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths():\n",
    "    \"\"\"\n",
    "    Get paths to all documents in the database. Ignore paths which are directories\n",
    "    Return: list of absolute file paths\n",
    "    \"\"\"\n",
    "    doc_paths = [f for f in glob.glob(PATH_TO_DATABASE + '/**', recursive=True) if not os.path.isdir(f)]\n",
    "    return [i.replace('\\\\', '/') for i in doc_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fragment_doc(doc):\n",
    "    \"\"\"\n",
    "    Split a document into paragraphs of sentences.\n",
    "    Returns a dict with info about the document, including the list of paragraphs.\n",
    "    \"\"\"\n",
    "    paragraphs = []\n",
    "    file_token_count = 0\n",
    "    # use blingfire sentence boundary detection to split document into sentences\n",
    "    sents = text_to_sentences(doc).split('\\n')\n",
    "    # combine sentences into paragraphs with overlap\n",
    "    for i in range(0, len(sents), CHUNK_SIZE-CHUNK_OVERLAP):\n",
    "        # using a slice, so even if [i:i+CHUNK_SIZE] is out of bounds, \n",
    "        # it will just return the remaining sentences\n",
    "        # this is useful for the last paragraph, which may not have CHUNK_SIZE sentences\n",
    "        para = ' '.join(sents[i:i+CHUNK_SIZE])\n",
    "        # check if paragraph is too long for the embedding model\n",
    "        file_num_tokens = len(enc.encode(para))\n",
    "        if file_num_tokens > EMBEDDING_TOKEN_LIMIT:\n",
    "            raise RuntimeError(f'{doc_name}: Paragraph too long: {para[:50]}...')\n",
    "        # paragraph is short enough, so just add it to the list\n",
    "        else:\n",
    "            paragraphs.append(para)\n",
    "        # update token count\n",
    "        file_token_count += file_num_tokens\n",
    "    # store info about the document in a dict\n",
    "    doc_data = {'num_sents': len(sents),\n",
    "                'num_paras': len(paragraphs),\n",
    "                'num_tokens': file_token_count,\n",
    "                'text': paragraphs}\n",
    "    return doc_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents():\n",
    "    \"\"\"\n",
    "    Read all documents in the database into a list of dicts.\n",
    "    Each dict contains info about the document, including the list of paragraphs.\n",
    "    \"\"\"\n",
    "    doc_paths = get_paths()\n",
    "    docs = []\n",
    "    for doc_path in doc_paths:\n",
    "        # read document\n",
    "        with open(doc_path, 'r', encoding='utf-8') as f:\n",
    "            doc = f.read()\n",
    "        doc_name = doc_path.split('/')[-1]\n",
    "        # split document into paragraphs\n",
    "        doc_data = fragment_doc(doc)\n",
    "        # add document name and path to dict\n",
    "        doc_data['doc_name'] = doc_name\n",
    "        doc_data['doc_path'] = doc_path\n",
    "        docs.append(doc_data)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(document_df):\n",
    "    \"\"\"Takes the prepared dataframe of documents and calculates metadata for entire database.\"\"\"\n",
    "    costs = {'num_docs': len(document_df),\n",
    "                 'num_paras': document_df['num_paras'].sum(),\n",
    "                 'num_tokens': document_df['num_tokens'].sum(),\n",
    "                 'est_cost_usd': np.round((document_df['num_tokens'].sum() * 0.0004 / 1000), 2)}\n",
    "    return costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_embedding(df):\n",
    "    \"\"\"\n",
    "    Explode the paragraphs into separate rows, generate a unique id for each paragraph.\n",
    "    Drop columns that are no longer needed.\n",
    "    Return: dataframe with one paragraph per row, and a unique id for each paragraph\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # explode the paragraphs into separate rows, generate a unique id for each paragraph\n",
    "    df = df.explode('text').reset_index(drop=True)\n",
    "    df = df.drop(columns=['num_sents', 'num_paras', 'num_tokens'])\n",
    "    df['id']= [uuid.uuid4().hex for _ in range(len(df))]\n",
    "    while len(df['id'].unique()) != len(df):\n",
    "        df['id']= [uuid.uuid4().hex for _ in range(len(df))]\n",
    "    # calculate tokens for each paragraph\n",
    "    df['num_tokens'] = df['text'].apply(lambda x: len(enc.encode(x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    try:\n",
    "        res = openai.Embedding.create(input = [text], model=EMBEDDING_MODEL)['data'][0]['embedding']\n",
    "    except:\n",
    "        res = np.zeros(EMBEDDING_LENGTH)\n",
    "        print(\"ERROR: Unable to embed text: \", text) # TODO: propagate error and handle it\n",
    "        logging.error(f'Unable to embed text: {text}')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_texts(df, col_name):\n",
    "    df = df.copy()\n",
    "    df['enriched_text'] = df['doc_name'] + ': ' + df[col_name]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_documents(df, col_name):\n",
    "    df = df.copy()\n",
    "    df['embedding'] = df[col_name].progress_apply(get_embedding)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents():\n",
    "    \"\"\"\n",
    "    Main function to ingest, embed, and store document data and embeddings.\n",
    "    \"\"\"\n",
    "    logging.info('process_documents(1/4): Attempting to process document database')\n",
    "    docs = read_documents()\n",
    "    df = pd.DataFrame(docs)\n",
    "    costs = calculate_cost(df)\n",
    "    print('documents loaded into memory')\n",
    "    print(f'details: {costs}')\n",
    "    logging.info(f'process_documents(2/4): Documents loaded into memory. Details: {costs}')\n",
    "\n",
    "    print('embedding documents...')\n",
    "    df = format_for_embedding(df)\n",
    "    df = enrich_texts(df, 'text')\n",
    "    df = embed_documents(df, 'enriched_text') # Note: this embeds the enriched text\n",
    "    logging.info('process_documents(3/4): Embedding complete')\n",
    "    # store df as pickle in data directory\n",
    "    df.to_pickle(os.getcwd() + '/data/embeddings.pkl')\n",
    "    print('embeddings dataframe saved to data/embeddings.pkl')\n",
    "    logging.info('process_documents(4/4): Successfully processed document database, data saved to data/embeddings.pkl')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - enrich each paragraph with the header and section name\n",
    "# this is context dependent, depends on how the text is written, and how chunking is done\n",
    "# decide whether to embed the enriched text, or just the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.pkl does not exist, processing documents...\n",
      "documents loaded into memory\n",
      "details: {'num_docs': 56, 'num_paras': 413, 'num_tokens': 77942, 'est_cost_usd': 0.03}\n",
      "dataframe prepared for embedding\n",
      "embedding documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 413/413 [02:52<00:00,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding complete\n",
      "embeddings dataframe saved to data/embeddings.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.getcwd() + '/data/embeddings.pkl'):\n",
    "    try: \n",
    "        df = pd.read_pickle(os.getcwd() + '/data/embeddings.pkl')\n",
    "        print('embeddings.pkl loaded into memory')\n",
    "    except:\n",
    "        print('embeddings.pkl exists, but could not be loaded into memory')\n",
    "        logging.critical('embeddings.pkl exists, but could not be loaded into memory')\n",
    "    costs = calculate_cost(pd.DataFrame(read_documents()))\n",
    "    print(f'to refresh the database, reprocessing all documents would cost: {costs[\"est_cost_usd\"]} USD')\n",
    "else:\n",
    "    print('embeddings.pkl does not exist, processing documents...')\n",
    "    process_documents()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query and semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbors(query_embedding, df, col_name, token_limit=1500, similarity_threshold=0.75, k=10):\n",
    "    \"\"\"\n",
    "    Find the nearest neighbors for a given embedding vector.\n",
    "    Uses cosine similarity to find the nearest neighbors.\n",
    "    Returns: dataframe with the nearest neighbors filtered by token limit, similarity threshold, and k\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # calculate cosine similarity between query embedding and all embeddings in df\n",
    "    df['similarity'] = df[col_name].apply(lambda x: cosine_similarity([query_embedding], [x])[0][0])\n",
    "    df = df.sort_values(by='similarity', ascending=False)\n",
    "    # apply constraints\n",
    "    constraints = {'k': k, \n",
    "                   'token_limit': sum(df.num_tokens.cumsum() < token_limit),\n",
    "                   'similarity_threshold': sum(df.similarity > similarity_threshold)}\n",
    "    # find the smallest constraint\n",
    "    constraint = min(constraints, key=constraints.get)\n",
    "    # slice df to include rows limited by the smallest constraint\n",
    "    df = df.iloc[:constraints[constraint]]      \n",
    "    \n",
    "    return df, constraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_prompt(prompt, context=None):\n",
    "    \"\"\"Converts a prompt into openai's message list format.\"\"\"\n",
    "\n",
    "    query =  f\"\"\"Use the below KNOWLEDGE BASE to answer the subsequent question. If KNOWLEDGE BASE is not relevant to the QUESTION, reply with only three words: \"I don't know.\".\n",
    "\n",
    "KNOWLEDGE BASE:\n",
    "\\\"\\\"\\\"\n",
    "{context}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "QUESTION: {prompt}\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You answer questions using the KNOWLEDGE BASE.\"},\n",
    "                {\"role\": \"user\", \"content\":query}]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(input):\n",
    "    \"\"\"Takes a user input and returns the answer from the knowledge base.\"\"\"\n",
    "    input_embedding = get_embedding(input)\n",
    "    results, constraint = nearest_neighbors(input_embedding, df, 'embedding')\n",
    "\n",
    "    # if results dataframe is empty, return a message saying no results found\n",
    "    if results.empty:\n",
    "        print(\"I don't know. No results found in database\")\n",
    "        return None, constraint\n",
    "    else:\n",
    "        context = results['enriched_text'].str.cat(sep='\\n')\n",
    "    # engineer prompt and get answer from model\n",
    "    messages = engineer_prompt(input, context)\n",
    "    try:\n",
    "        result = openai.ChatCompletion.create(model=ACTIVE_MODEL, messages=messages, temperature=0)\n",
    "    except:\n",
    "        print('ERROR: Please try again later') # TO DO: catch more specific errors https://platform.openai.com/docs/guides/error-codes/python-library-error-types\n",
    "        return None, constraint\n",
    "\n",
    "    return result, constraint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "think about the prompt function\n",
    "it needs to return not only the answer but also log the results of the search\n",
    "returning the constraint is also useful (can recommend to the user, that there are not enough results, or that they should be more precise)\n",
    "\n",
    "prompt has three return types:\n",
    "\n",
    "no search results - None, constraint\n",
    "completion failed - None, constraint\n",
    "completion succeeded - answer, constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milo dinosaur is a Malaysian/Singaporean beverage, composed of a cup of iced Milo with undissolved Milo powder added on top of it. The drink is made by adding a generous amount of undissolved Milo powder to a cup of iced Milo.\n",
      "Results limited by: token_limit\n"
     ]
    }
   ],
   "source": [
    "user_input = \"How does one drink a dinosaur?\"\n",
    "\n",
    "answer, constraint = prompt(user_input)\n",
    "\n",
    "print(answer.choices[0]['message']['content'])\n",
    "print(f'Results limited by: {constraint}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log input, answer, and constraint\n",
    "log = {'input': user_input, 'answer': answer.choices[0]['message']['content'], 'constraint': constraint}\n",
    "logging.info(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7IMS30aDVKGg5HMCEa9azKyFpnvrL at 0x1810d1899f0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"Milo dinosaur is a Malaysian/Singaporean beverage, composed of a cup of iced Milo with undissolved Milo powder added on top of it. The drink is made by adding a generous amount of undissolved Milo powder to a cup of iced Milo.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1684610563,\n",
       "  \"id\": \"chatcmpl-7IMS30aDVKGg5HMCEa9azKyFpnvrL\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 55,\n",
       "    \"prompt_tokens\": 1562,\n",
       "    \"total_tokens\": 1617\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milo dinosaur is a Malaysian/Singaporean beverage, composed of a cup of iced Milo with undissolved Milo powder added on top of it. The drink is made by adding a generous amount of undissolved Milo powder to a cup of iced Milo.\n"
     ]
    }
   ],
   "source": [
    "print(answer.choices[0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7IMN83ToyXE8L3U4vN3BhPgACPqe7 at 0x1810d189e50> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"Milo dinosaur is a Malaysian/Singaporean beverage, composed of a cup of iced Milo with undissolved Milo powder added on top of it. The drink is made by adding a generous amount of undissolved Milo powder to a cup of iced Milo.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1684610258,\n",
       "  \"id\": \"chatcmpl-7IMN83ToyXE8L3U4vN3BhPgACPqe7\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 55,\n",
       "    \"prompt_tokens\": 1562,\n",
       "    \"total_tokens\": 1617\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babel_venv_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
