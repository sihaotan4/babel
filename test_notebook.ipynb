{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sihao\\Documents\\projects\\babel\\venv_babel\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import datetime\n",
    "from time import sleep\n",
    "import openai\n",
    "import tiktoken\n",
    "from blingfire import text_to_sentences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths():\n",
    "    \"\"\"\n",
    "    Get paths to all documents in the database. Ignore paths which are directories\n",
    "    Return: list of absolute file paths\n",
    "    \"\"\"\n",
    "    doc_paths = [f for f in glob.glob(PATH_TO_DATABASE + '/**', recursive=True) if not os.path.isdir(f)]\n",
    "    return [i.replace('\\\\', '/') for i in doc_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fragment_doc(doc):\n",
    "    \"\"\"\n",
    "    Split a document into paragraphs of sentences.\n",
    "    Returns a dict with info about the document, including the list of paragraphs.\n",
    "    \"\"\"\n",
    "    paragraphs = []\n",
    "    file_token_count = 0\n",
    "    # use blingfire sentence boundary detection to split document into sentences\n",
    "    sents = text_to_sentences(doc).split('\\n')\n",
    "    # combine sentences into paragraphs with overlap\n",
    "    for i in range(0, len(sents), CHUNK_SIZE-CHUNK_OVERLAP):\n",
    "        # using a slice, so even if [i:i+CHUNK_SIZE] is out of bounds, \n",
    "        # it will just return the remaining sentences\n",
    "        # this is useful for the last paragraph, which may not have CHUNK_SIZE sentences\n",
    "        para = ' '.join(sents[i:i+CHUNK_SIZE])\n",
    "        # check if paragraph is too long for the embedding model\n",
    "        file_num_tokens = len(enc.encode(para))\n",
    "        if file_num_tokens > EMBEDDING_TOKEN_LIMIT:\n",
    "            raise RuntimeError(f'{doc_name}: Paragraph too long: {para[:50]}...')\n",
    "        # paragraph is short enough, so just add it to the list\n",
    "        else:\n",
    "            paragraphs.append(para)\n",
    "        # update token count\n",
    "        file_token_count += file_num_tokens\n",
    "    # store info about the document in a dict\n",
    "    doc_data = {'num_sents': len(sents),\n",
    "                'num_paras': len(paragraphs),\n",
    "                'num_tokens': file_token_count,\n",
    "                'text': paragraphs}\n",
    "    return doc_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents():\n",
    "    \"\"\"\n",
    "    Read all documents in the database into a list of dicts.\n",
    "    Each dict contains info about the document, including the list of paragraphs.\n",
    "    \"\"\"\n",
    "    doc_paths = get_paths()\n",
    "    docs = []\n",
    "    for doc_path in doc_paths:\n",
    "        # read document\n",
    "        with open(doc_path, 'r', encoding='utf-8') as f:\n",
    "            doc = f.read()\n",
    "        doc_name = doc_path.split('/')[-1]\n",
    "        # split document into paragraphs\n",
    "        doc_data = fragment_doc(doc)\n",
    "        # add document name and path to dict\n",
    "        doc_data['doc_name'] = doc_name\n",
    "        doc_data['doc_path'] = doc_path\n",
    "        docs.append(doc_data)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(document_df):\n",
    "    \"\"\"Takes the prepared dataframe of documents and calculates metadata for entire database.\"\"\"\n",
    "    costs = {'num_docs': len(document_df),\n",
    "                 'num_paras': document_df['num_paras'].sum(),\n",
    "                 'num_tokens': document_df['num_tokens'].sum(),\n",
    "                 'est_cost_usd': np.round((document_df['num_tokens'].sum() * 0.0004 / 1000), 2)}\n",
    "    return costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_embedding(df):\n",
    "    \"\"\"\n",
    "    Explode the paragraphs into separate rows, generate a unique id for each paragraph.\n",
    "    Drop columns that are no longer needed.\n",
    "    Return: dataframe with one paragraph per row, and a unique id for each paragraph\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # explode the paragraphs into separate rows, generate a unique id for each paragraph\n",
    "    df = df.explode('text').reset_index(drop=True)\n",
    "    df = df.drop(columns=['num_sents', 'num_paras', 'num_tokens'])\n",
    "    df['id']= [uuid.uuid4().hex for _ in range(len(df))]\n",
    "    while len(df['id'].unique()) != len(df):\n",
    "        df['id']= [uuid.uuid4().hex for _ in range(len(df))]\n",
    "    # calculate tokens for each paragraph\n",
    "    df['num_tokens'] = df['text'].apply(lambda x: len(enc.encode(x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    try:\n",
    "        res = openai.Embedding.create(input = [text], model=EMBEDDING_MODEL)['data'][0]['embedding']\n",
    "    except:\n",
    "        res = np.zeros(EMBEDDING_LENGTH)\n",
    "        print(\"ERROR: Unable to embed text: \", text) # TODO: propagate error and handle it\n",
    "        logging.error(f'Unable to embed text: {text}')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_texts(df, col_name):\n",
    "    df = df.copy()\n",
    "    df['enriched_text'] = df['doc_name'] + ': ' + df[col_name]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_documents(df, col_name):\n",
    "    df = df.copy()\n",
    "    df['embedding'] = df[col_name].progress_apply(get_embedding)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents():\n",
    "    \"\"\"\n",
    "    Main function to ingest, embed, and store document data and embeddings.\n",
    "    \"\"\"\n",
    "    logging.debug('process_documents(1/4): Attempting to process document database')\n",
    "    docs = read_documents()\n",
    "    df = pd.DataFrame(docs)\n",
    "    costs = calculate_cost(df)\n",
    "    print('documents loaded into memory')\n",
    "    print(f'details: {costs}')\n",
    "    logging.debug(f'process_documents(2/4): Documents loaded into memory. Details: {costs}')\n",
    "\n",
    "    print('embedding documents...')\n",
    "    df = format_for_embedding(df)\n",
    "    df = enrich_texts(df, 'text')\n",
    "    df = embed_documents(df, 'enriched_text') # Note: this embeds the enriched text\n",
    "    logging.debug('process_documents(3/4): Embedding complete')\n",
    "    # store df as pickle in data directory\n",
    "    df.to_pickle(os.getcwd() + '/data/embeddings.pkl')\n",
    "    print('embeddings dataframe saved to data/embeddings.pkl')\n",
    "    logging.debug('process_documents(4/4): Successfully processed document database, data saved to data/embeddings.pkl')\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - enrich each paragraph with the header and section name\n",
    "this is context dependent, depends on how the text is written, and how chunking is done\n",
    "decide whether to embed the enriched text, or just the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbors(query_embedding, df, col_name, token_limit=1500, similarity_threshold=0.75, k=10):\n",
    "    \"\"\"\n",
    "    Find the nearest neighbors for a given embedding vector.\n",
    "    Uses cosine similarity to find the nearest neighbors.\n",
    "    Returns: dataframe with the nearest neighbors filtered by token limit, similarity threshold, and k\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # calculate cosine similarity between query embedding and all embeddings in df\n",
    "    df['similarity'] = df[col_name].apply(lambda x: cosine_similarity([query_embedding], [x])[0][0])\n",
    "    df = df.sort_values(by='similarity', ascending=False)\n",
    "    # apply constraints\n",
    "    constraints = {'k': k, \n",
    "                   'token_limit': sum(df.num_tokens.cumsum() < token_limit),\n",
    "                   'similarity_threshold': sum(df.similarity > similarity_threshold)}\n",
    "    # find the smallest constraint\n",
    "    constraint = min(constraints, key=constraints.get)\n",
    "    # slice df to include rows limited by the smallest constraint\n",
    "    df = df.iloc[:constraints[constraint]]      \n",
    "    \n",
    "    return df, constraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_prompt(prompt, context=None):\n",
    "    \"\"\"Converts a prompt into openai's message list format.\"\"\"\n",
    "\n",
    "    query =  f\"\"\"Use the below KNOWLEDGE BASE to answer the subsequent question. If KNOWLEDGE BASE is not relevant to the QUESTION, reply with only three words: \"I don't know.\".\n",
    "\n",
    "KNOWLEDGE BASE:\n",
    "\\\"\\\"\\\"\n",
    "{context}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "QUESTION: {prompt}\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You answer questions using the KNOWLEDGE BASE.\"},\n",
    "                {\"role\": \"user\", \"content\": query}]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(input):\n",
    "    \"\"\"Takes a user input and returns the answer from the knowledge base.\"\"\"\n",
    "    logging.info(f'get_answer(1/2): received input: {input}')\n",
    "    input_embedding = get_embedding(input)\n",
    "    results, constraint = nearest_neighbors(input_embedding, df, 'embedding')\n",
    "\n",
    "    # if results dataframe is empty, return None\n",
    "    if results.empty:\n",
    "        logging.info(f'get_answer(2/2): No results due to constraint:{constraint}')\n",
    "        print(f'No results found. Please try again with a different input.')\n",
    "        return None, constraint\n",
    "    else:\n",
    "        context = results['enriched_text'].str.cat(sep='\\n')\n",
    "    \n",
    "    # get answer from model with engineered prompt\n",
    "    messages = engineer_prompt(input, context)\n",
    "    try:\n",
    "        answer = openai.ChatCompletion.create(model=ACTIVE_MODEL, messages=messages, temperature=0)\n",
    "    except:\n",
    "        error_code = answer['error']['code']\n",
    "        print(f'Please try again later. Error code: {error_code} from openai')\n",
    "        logging.info(f'get_answer(2/2): Unsuccessful. Error code: {error_code} from openai.ChatCompletion.create()')\n",
    "        return None, constraint\n",
    "    \n",
    "    logging.info(f'get_answer(2/2): Successful. Details: {json.dumps(answer)}')\n",
    "    return answer, constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # initialize global variables and logging\n",
    "    PATH_TO_DATABASE = os.getcwd() + '/documents'\n",
    "    CHUNK_SIZE = 5 # number of sentences per paragraph\n",
    "    CHUNK_OVERLAP = 1 # number of sentences to overlap between paragraphs \n",
    "\n",
    "    EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "    EMBEDDING_TOKEN_LIMIT = 8191-2 # account for the 2 tokens added by the model\n",
    "    EMBEDDING_LENGTH = 1536 # length of the embedding vector\n",
    "    EMBEDDING_BATCH_SIZE = 25 # number of paras to embed at once\n",
    "\n",
    "    ACTIVE_MODEL = \"gpt-3.5-turbo\"\n",
    "    ACTIVE_TOKEN_LIMIT = 4096-2\n",
    "\n",
    "    logging.basicConfig(filename=f'{os.getcwd()}/logs/babel.log', \n",
    "                                    format='%(asctime)s-%(levelname)s-%(message)s', \n",
    "                                    level=logging.INFO)\n",
    "    with open(os.getcwd() + '/secrets/keys.json') as f:\n",
    "        keys = json.load(f)\n",
    "    openai.api_key = keys['OPENAI_API_KEY']\n",
    "    enc = tiktoken.encoding_for_model(EMBEDDING_MODEL)\n",
    "\n",
    "    # load embeddings into memory\n",
    "    if os.path.exists(os.getcwd() + '/data/embeddings.pkl'):\n",
    "        try: \n",
    "            df = pd.read_pickle(os.getcwd() + '/data/embeddings.pkl')\n",
    "            print('embeddings.pkl loaded into memory')\n",
    "        except:\n",
    "            print('embeddings.pkl exists, but could not be loaded into memory')\n",
    "            logging.critical('embeddings.pkl exists, but could not be loaded into memory')\n",
    "        costs = calculate_cost(pd.DataFrame(read_documents()))\n",
    "        print(f'to refresh the database, reprocessing all documents would cost: {costs[\"est_cost_usd\"]} USD')\n",
    "    else:\n",
    "        print('embeddings.pkl does not exist, processing documents...')\n",
    "        process_documents()\n",
    "\n",
    "    # Q & A loop\n",
    "    user_input = \"How does one drink a dinosaur?\"\n",
    "    answer, constraint = get_answer(user_input)\n",
    "    if answer:\n",
    "        print(answer.choices[0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.pkl loaded into memory\n",
      "to refresh the database, reprocessing all documents would cost: 0.03 USD\n"
     ]
    }
   ],
   "source": [
    "# initialize global variables and logging\n",
    "PATH_TO_DATABASE = os.getcwd() + '/documents'\n",
    "CHUNK_SIZE = 5 # number of sentences per paragraph\n",
    "CHUNK_OVERLAP = 1 # number of sentences to overlap between paragraphs \n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "EMBEDDING_TOKEN_LIMIT = 8191-2 # account for the 2 tokens added by the model\n",
    "EMBEDDING_LENGTH = 1536 # length of the embedding vector\n",
    "EMBEDDING_BATCH_SIZE = 25 # number of paras to embed at once\n",
    "\n",
    "ACTIVE_MODEL = \"gpt-3.5-turbo\"\n",
    "ACTIVE_TOKEN_LIMIT = 4096-2\n",
    "\n",
    "logging.basicConfig(filename=f'{os.getcwd()}/logs/babel.log', \n",
    "                                format='%(asctime)s-%(levelname)s-%(message)s', \n",
    "                                level=logging.INFO)\n",
    "with open(os.getcwd() + '/secrets/keys.json') as f:\n",
    "    keys = json.load(f)\n",
    "openai.api_key = keys['OPENAI_API_KEY']\n",
    "enc = tiktoken.encoding_for_model(EMBEDDING_MODEL)\n",
    "\n",
    "# load embeddings into memory\n",
    "if os.path.exists(os.getcwd() + '/data/embeddings.pkl'):\n",
    "    try: \n",
    "        df = pd.read_pickle(os.getcwd() + '/data/embeddings.pkl')\n",
    "        print('embeddings.pkl loaded into memory')\n",
    "    except:\n",
    "        print('embeddings.pkl exists, but could not be loaded into memory')\n",
    "        logging.critical('embeddings.pkl exists, but could not be loaded into memory')\n",
    "    costs = calculate_cost(pd.DataFrame(read_documents()))\n",
    "    print(f'to refresh the database, reprocessing all documents would cost: {costs[\"est_cost_usd\"]} USD')\n",
    "else:\n",
    "    print('embeddings.pkl does not exist, processing documents...')\n",
    "    process_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"How does one drink a dinosaur?\"\n",
    "\n",
    "input_embedding = get_embedding(input)\n",
    "results, constraint = nearest_neighbors(input_embedding, df, 'embedding')\n",
    "\n",
    "# if results dataframe is empty, return None\n",
    "if results.empty:\n",
    "    logging.info(f'get_answer(2/2): No results due to constraint:{constraint}')\n",
    "    print(f'No results found. Please try again with a different input.')\n",
    "    return None, constraint\n",
    "else:\n",
    "    context = results['enriched_text'].str.cat(sep='\\n')\n",
    "\n",
    "# get answer from model with engineered prompt\n",
    "messages = engineer_prompt(input, context)\n",
    "try:\n",
    "    answer = openai.ChatCompletion.create(model=ACTIVE_MODEL, messages=messages, temperature=0)\n",
    "except:\n",
    "    error_code = answer['error']['code']\n",
    "    print(f'Please try again later. Error code: {error_code} from openai')\n",
    "    return None, constraint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"How does one drink a dinosaur?\"\n",
    "\n",
    "input_embedding = get_embedding(input)\n",
    "results, constraint = nearest_neighbors(input_embedding, df, 'embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>doc_name</th>\n",
       "      <th>doc_path</th>\n",
       "      <th>id</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>enriched_text</th>\n",
       "      <th>embedding</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Milo dinosaur or Milo tabur is a Malaysian/Sin...</td>\n",
       "      <td>Milo dinosaur</td>\n",
       "      <td>C:/Users/sihao/Documents/projects/babel/docume...</td>\n",
       "      <td>2467e556cb4e4acd80f5a43b6d4a7a1e</td>\n",
       "      <td>190</td>\n",
       "      <td>Milo dinosaur: Milo dinosaur or Milo tabur is ...</td>\n",
       "      <td>[0.00023811885330360383, -0.030089786276221275...</td>\n",
       "      <td>0.828739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>The name of the beverage is one of the terms t...</td>\n",
       "      <td>Milo dinosaur</td>\n",
       "      <td>C:/Users/sihao/Documents/projects/babel/docume...</td>\n",
       "      <td>3c2fec6aad694c838e9cc75894a23e5c</td>\n",
       "      <td>90</td>\n",
       "      <td>Milo dinosaur: The name of the beverage is one...</td>\n",
       "      <td>[-0.00988902896642685, -0.031585853546857834, ...</td>\n",
       "      <td>0.827594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>The amount of powder added can be excessive or...</td>\n",
       "      <td>Milo dinosaur</td>\n",
       "      <td>C:/Users/sihao/Documents/projects/babel/docume...</td>\n",
       "      <td>bbd6564679494a25afe1533ef3a738d6</td>\n",
       "      <td>172</td>\n",
       "      <td>Milo dinosaur: The amount of powder added can ...</td>\n",
       "      <td>[0.002130742883309722, -0.02586507983505726, -...</td>\n",
       "      <td>0.827039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>At present, Malaysia has the world's highest p...</td>\n",
       "      <td>Milo dinosaur</td>\n",
       "      <td>C:/Users/sihao/Documents/projects/babel/docume...</td>\n",
       "      <td>6ba00dd43bbe43de8d9de9b53bc73594</td>\n",
       "      <td>92</td>\n",
       "      <td>Milo dinosaur: At present, Malaysia has the wo...</td>\n",
       "      <td>[0.013058049604296684, -0.015157836489379406, ...</td>\n",
       "      <td>0.816668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>Giles Rose made turtle soup as follows: \"Take ...</td>\n",
       "      <td>Turtle soup</td>\n",
       "      <td>C:/Users/sihao/Documents/projects/babel/docume...</td>\n",
       "      <td>e255d1e5d5404ad78f0bb72a62d06029</td>\n",
       "      <td>270</td>\n",
       "      <td>Turtle soup: Giles Rose made turtle soup as fo...</td>\n",
       "      <td>[0.036201100796461105, -0.008135446347296238, ...</td>\n",
       "      <td>0.776234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text       doc_name  \\\n",
       "60   Milo dinosaur or Milo tabur is a Malaysian/Sin...  Milo dinosaur   \n",
       "63   The name of the beverage is one of the terms t...  Milo dinosaur   \n",
       "62   The amount of powder added can be excessive or...  Milo dinosaur   \n",
       "61   At present, Malaysia has the world's highest p...  Milo dinosaur   \n",
       "265  Giles Rose made turtle soup as follows: \"Take ...    Turtle soup   \n",
       "\n",
       "                                              doc_path  \\\n",
       "60   C:/Users/sihao/Documents/projects/babel/docume...   \n",
       "63   C:/Users/sihao/Documents/projects/babel/docume...   \n",
       "62   C:/Users/sihao/Documents/projects/babel/docume...   \n",
       "61   C:/Users/sihao/Documents/projects/babel/docume...   \n",
       "265  C:/Users/sihao/Documents/projects/babel/docume...   \n",
       "\n",
       "                                   id  num_tokens  \\\n",
       "60   2467e556cb4e4acd80f5a43b6d4a7a1e         190   \n",
       "63   3c2fec6aad694c838e9cc75894a23e5c          90   \n",
       "62   bbd6564679494a25afe1533ef3a738d6         172   \n",
       "61   6ba00dd43bbe43de8d9de9b53bc73594          92   \n",
       "265  e255d1e5d5404ad78f0bb72a62d06029         270   \n",
       "\n",
       "                                         enriched_text  \\\n",
       "60   Milo dinosaur: Milo dinosaur or Milo tabur is ...   \n",
       "63   Milo dinosaur: The name of the beverage is one...   \n",
       "62   Milo dinosaur: The amount of powder added can ...   \n",
       "61   Milo dinosaur: At present, Malaysia has the wo...   \n",
       "265  Turtle soup: Giles Rose made turtle soup as fo...   \n",
       "\n",
       "                                             embedding  similarity  \n",
       "60   [0.00023811885330360383, -0.030089786276221275...    0.828739  \n",
       "63   [-0.00988902896642685, -0.031585853546857834, ...    0.827594  \n",
       "62   [0.002130742883309722, -0.02586507983505726, -...    0.827039  \n",
       "61   [0.013058049604296684, -0.015157836489379406, ...    0.816668  \n",
       "265  [0.036201100796461105, -0.008135446347296238, ...    0.776234  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble a string containing all the doc_name and id from results\n",
    "citations = ''\n",
    "for row in results.itertuples():\n",
    "    citations += f'{row.doc_name} - {row.id}\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Milo dinosaur - 2467e556cb4e4acd80f5a43b6d4a7a1e\\nMilo dinosaur - 3c2fec6aad694c838e9cc75894a23e5c\\nMilo dinosaur - bbd6564679494a25afe1533ef3a738d6\\nMilo dinosaur - 6ba00dd43bbe43de8d9de9b53bc73594\\nTurtle soup - e255d1e5d5404ad78f0bb72a62d06029\\nTeh tarik - dd16f78e28a549e5b2e1d7475d39949d\\nDrunken prawn - 3d3264a7a4c84fa38de470e778fdedbd\\nTurtle soup - 1340d69bf4874e67b326e984de0e465a\\nTurtle soup - 9a315cf7b78849f594c7c5d1424c29eb\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babel_venv_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
