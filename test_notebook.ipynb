{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sihao\\Documents\\projects\\babel\\venv_babel\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import datetime\n",
    "from time import sleep\n",
    "import openai\n",
    "import tiktoken\n",
    "from blingfire import text_to_sentences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths():\n",
    "    \"\"\"\n",
    "    Get paths to all documents in the database. Ignore paths which are directories\n",
    "    Return: list of absolute file paths\n",
    "    \"\"\"\n",
    "    doc_paths = [f for f in glob.glob(PATH_TO_DATABASE + '/**', recursive=True) if not os.path.isdir(f)]\n",
    "    return [i.replace('\\\\', '/') for i in doc_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fragment_doc(doc):\n",
    "    \"\"\"\n",
    "    Split a document into paragraphs of sentences.\n",
    "    Returns a dict with info about the document, including the list of paragraphs.\n",
    "    \"\"\"\n",
    "    paragraphs = []\n",
    "    file_token_count = 0\n",
    "    # use blingfire sentence boundary detection to split document into sentences\n",
    "    sents = text_to_sentences(doc).split('\\n')\n",
    "    # combine sentences into paragraphs with overlap\n",
    "    for i in range(0, len(sents), CHUNK_SIZE-CHUNK_OVERLAP):\n",
    "        # using a slice, so even if [i:i+CHUNK_SIZE] is out of bounds, \n",
    "        # it will just return the remaining sentences\n",
    "        # this is useful for the last paragraph, which may not have CHUNK_SIZE sentences\n",
    "        para = ' '.join(sents[i:i+CHUNK_SIZE])\n",
    "        # check if paragraph is too long for the embedding model\n",
    "        file_num_tokens = len(enc.encode(para))\n",
    "        if file_num_tokens > EMBEDDING_TOKEN_LIMIT:\n",
    "            raise RuntimeError(f'{doc_name}: Paragraph too long: {para[:50]}...')\n",
    "        # paragraph is short enough, so just add it to the list\n",
    "        else:\n",
    "            paragraphs.append(para)\n",
    "        # update token count\n",
    "        file_token_count += file_num_tokens\n",
    "    # store info about the document in a dict\n",
    "    doc_data = {'num_sents': len(sents),\n",
    "                'num_paras': len(paragraphs),\n",
    "                'num_tokens': file_token_count,\n",
    "                'text': paragraphs}\n",
    "    return doc_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents():\n",
    "    \"\"\"\n",
    "    Read all documents in the database into a list of dicts.\n",
    "    Each dict contains info about the document, including the list of paragraphs.\n",
    "    \"\"\"\n",
    "    doc_paths = get_paths()\n",
    "    docs = []\n",
    "    for doc_path in doc_paths:\n",
    "        # read document\n",
    "        with open(doc_path, 'r', encoding='utf-8') as f:\n",
    "            doc = f.read()\n",
    "        doc_name = doc_path.split('/')[-1]\n",
    "        # split document into paragraphs\n",
    "        doc_data = fragment_doc(doc)\n",
    "        # add document name and path to dict\n",
    "        doc_data['doc_name'] = doc_name\n",
    "        doc_data['doc_path'] = doc_path\n",
    "        docs.append(doc_data)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(document_df):\n",
    "    \"\"\"Takes the prepared dataframe of documents and calculates metadata for entire database.\"\"\"\n",
    "    costs = {'num_docs': len(document_df),\n",
    "                 'num_paras': document_df['num_paras'].sum(),\n",
    "                 'num_tokens': document_df['num_tokens'].sum(),\n",
    "                 'est_cost_usd': np.round((document_df['num_tokens'].sum() * 0.0004 / 1000), 2)}\n",
    "    return costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_embedding(df):\n",
    "    \"\"\"\n",
    "    Explode the paragraphs into separate rows, generate a unique id for each paragraph.\n",
    "    Drop columns that are no longer needed.\n",
    "    Return: dataframe with one paragraph per row, and a unique id for each paragraph\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # explode the paragraphs into separate rows, generate a unique id for each paragraph\n",
    "    df = df.explode('text').reset_index(drop=True)\n",
    "    df = df.drop(columns=['num_sents', 'num_paras', 'num_tokens'])\n",
    "    df['id']= [uuid.uuid4().hex for _ in range(len(df))]\n",
    "    while len(df['id'].unique()) != len(df):\n",
    "        df['id']= [uuid.uuid4().hex for _ in range(len(df))]\n",
    "    # calculate tokens for each paragraph\n",
    "    df['num_tokens'] = df['text'].apply(lambda x: len(enc.encode(x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    try:\n",
    "        res = openai.Embedding.create(input = [text], model=EMBEDDING_MODEL)['data'][0]['embedding']\n",
    "    except:\n",
    "        res = np.zeros(EMBEDDING_LENGTH)\n",
    "        print(\"ERROR: Unable to embed text: \", text) # TODO: propagate error and handle it\n",
    "        logging.error(f'Unable to embed text: {text}')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_texts(df, col_name):\n",
    "    df = df.copy()\n",
    "    df['enriched_text'] = df['doc_name'] + ': ' + df[col_name]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_documents(df, col_name):\n",
    "    df = df.copy()\n",
    "    df['embedding'] = df[col_name].progress_apply(get_embedding)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents():\n",
    "    \"\"\"\n",
    "    Main function to ingest, embed, and store document data and embeddings.\n",
    "    \"\"\"\n",
    "    logging.debug('process_documents(1/4): Attempting to process document database')\n",
    "    docs = read_documents()\n",
    "    df = pd.DataFrame(docs)\n",
    "    costs = calculate_cost(df)\n",
    "    print('documents loaded into memory')\n",
    "    print(f'details: {costs}')\n",
    "    logging.debug(f'process_documents(2/4): Documents loaded into memory. Details: {costs}')\n",
    "\n",
    "    print('embedding documents...')\n",
    "    df = format_for_embedding(df)\n",
    "    df = enrich_texts(df, 'text')\n",
    "    df = embed_documents(df, 'enriched_text') # Note: this embeds the enriched text\n",
    "    logging.debug('process_documents(3/4): Embedding complete')\n",
    "    # store df as pickle in data directory\n",
    "    df.to_pickle(os.getcwd() + '/data/embeddings.pkl')\n",
    "    print('embeddings dataframe saved to data/embeddings.pkl')\n",
    "    logging.debug('process_documents(4/4): Successfully processed document database, data saved to data/embeddings.pkl')\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - enrich each paragraph with the header and section name\n",
    "this is context dependent, depends on how the text is written, and how chunking is done\n",
    "decide whether to embed the enriched text, or just the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbors(query_embedding, df, col_name, token_limit=1500, similarity_threshold=0.75, k=10):\n",
    "    \"\"\"\n",
    "    Find the nearest neighbors for a given embedding vector.\n",
    "    Uses cosine similarity to find the nearest neighbors.\n",
    "    Returns: dataframe with the nearest neighbors filtered by token limit, similarity threshold, and k\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # calculate cosine similarity between query embedding and all embeddings in df\n",
    "    df['similarity'] = df[col_name].apply(lambda x: cosine_similarity([query_embedding], [x])[0][0])\n",
    "    df = df.sort_values(by='similarity', ascending=False)\n",
    "    # apply constraints\n",
    "    constraints = {'k': k, \n",
    "                   'token_limit': sum(df.num_tokens.cumsum() < token_limit),\n",
    "                   'similarity_threshold': sum(df.similarity > similarity_threshold)}\n",
    "    # find the smallest constraint\n",
    "    constraint = min(constraints, key=constraints.get)\n",
    "    # slice df to include rows limited by the smallest constraint\n",
    "    df = df.iloc[:constraints[constraint]]      \n",
    "    \n",
    "    return df, constraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_prompt(prompt, context=None):\n",
    "    \"\"\"Converts a prompt into openai's message list format.\"\"\"\n",
    "\n",
    "    query =  f\"\"\"Use the below KNOWLEDGE BASE to answer the subsequent question. If KNOWLEDGE BASE is not relevant to the QUESTION, reply with only three words: \"I don't know.\".\n",
    "\n",
    "KNOWLEDGE BASE:\n",
    "\\\"\\\"\\\"\n",
    "{context}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "QUESTION: {prompt}\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You answer questions using the KNOWLEDGE BASE.\"},\n",
    "                {\"role\": \"user\", \"content\": query}]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(input):\n",
    "    \"\"\"Takes a user input and returns the answer from the knowledge base.\"\"\"\n",
    "    logging.info(f'get_answer(1/2): received input: {input}')\n",
    "    input_embedding = get_embedding(input)\n",
    "    results, constraint = nearest_neighbors(input_embedding, df, 'embedding')\n",
    "\n",
    "    # if results dataframe is empty, return None\n",
    "    if results.empty:\n",
    "        logging.info(f'get_answer(2/2): No results due to constraint:{constraint}')\n",
    "        print(f'No results found. Please try again with a different input.')\n",
    "        return None, constraint\n",
    "    else:\n",
    "        context = results['enriched_text'].str.cat(sep='\\n')\n",
    "    \n",
    "    # get answer from model with engineered prompt\n",
    "    messages = engineer_prompt(input, context)\n",
    "    try:\n",
    "        answer = openai.ChatCompletion.create(model=ACTIVE_MODEL, messages=messages, temperature=0)\n",
    "    except:\n",
    "        error_code = answer['error']['code']\n",
    "        print(f'Please try again later. Error code: {error_code} from openai')\n",
    "        logging.info(f'get_answer(2/2): Unsuccessful. Error code: {error_code} from openai.ChatCompletion.create()')\n",
    "        return None, constraint\n",
    "    \n",
    "    logging.info(f'get_answer(2/2): Successful. Details: {json.dumps(answer)}')\n",
    "    return answer, constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # initialize global variables and logging\n",
    "    PATH_TO_DATABASE = os.getcwd() + '/documents'\n",
    "    CHUNK_SIZE = 5 # number of sentences per paragraph\n",
    "    CHUNK_OVERLAP = 1 # number of sentences to overlap between paragraphs \n",
    "\n",
    "    EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "    EMBEDDING_TOKEN_LIMIT = 8191-2 # account for the 2 tokens added by the model\n",
    "    EMBEDDING_LENGTH = 1536 # length of the embedding vector\n",
    "    EMBEDDING_BATCH_SIZE = 25 # number of paras to embed at once\n",
    "\n",
    "    ACTIVE_MODEL = \"gpt-3.5-turbo\"\n",
    "    ACTIVE_TOKEN_LIMIT = 4096-2\n",
    "\n",
    "    logging.basicConfig(filename=f'{os.getcwd()}/logs/babel.log', \n",
    "                                    format='%(asctime)s-%(levelname)s-%(message)s', \n",
    "                                    level=logging.INFO)\n",
    "    with open(os.getcwd() + '/secrets/keys.json') as f:\n",
    "        keys = json.load(f)\n",
    "    openai.api_key = keys['OPENAI_API_KEY']\n",
    "    enc = tiktoken.encoding_for_model(EMBEDDING_MODEL)\n",
    "\n",
    "    # load embeddings into memory\n",
    "    if os.path.exists(os.getcwd() + '/data/embeddings.pkl'):\n",
    "        try: \n",
    "            df = pd.read_pickle(os.getcwd() + '/data/embeddings.pkl')\n",
    "            print('embeddings.pkl loaded into memory')\n",
    "        except:\n",
    "            print('embeddings.pkl exists, but could not be loaded into memory')\n",
    "            logging.critical('embeddings.pkl exists, but could not be loaded into memory')\n",
    "        costs = calculate_cost(pd.DataFrame(read_documents()))\n",
    "        print(f'to refresh the database, reprocessing all documents would cost: {costs[\"est_cost_usd\"]} USD')\n",
    "    else:\n",
    "        print('embeddings.pkl does not exist, processing documents...')\n",
    "        process_documents()\n",
    "\n",
    "    # Q & A loop\n",
    "    user_input = \"How does one drink a dinosaur?\"\n",
    "    answer, constraint = get_answer(user_input)\n",
    "    if answer:\n",
    "        print(answer.choices[0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babel_venv_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
